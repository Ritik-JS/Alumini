# üöÄ AI Systems Implementation Roadmap

## Executive Summary

This roadmap provides a **phased implementation plan** for integrating 6 AI systems and the Admin Dataset Upload feature into the AlumUnity backend. The implementation is designed to be incremental, allowing for testing and validation at each phase.

---

## üìä Implementation Phases Overview

| Phase | Duration | Focus | Complexity | Priority |
|-------|----------|-------|------------|----------|
| **Phase 1** | 2-3 weeks | Infrastructure + Admin Upload | Medium | CRITICAL |
| **Phase 2** | 2-3 weeks | Skill Graph AI + Engagement Scoring | Medium | HIGH |
| **Phase 3** | 3-4 weeks | Career Path Prediction + Talent Heatmap | High | HIGH |
| **Phase 4** | 2-3 weeks | Alumni ID Validation + Capsule Ranking | Medium | MEDIUM |
| **Phase 5** | 1-2 weeks | Optimization + Testing | Low | HIGH |
| **Total** | **10-15 weeks** | **Full AI Integration** | - | - |

---

## üèóÔ∏è PHASE 1: Infrastructure Setup & Admin Dataset Upload (Weeks 1-3)

### Objectives
- Set up core infrastructure (Redis, Celery, MySQL)
- Implement Admin Dataset Upload System
- Create data validation and cleaning pipeline
- Establish background job processing

### Deliverables

#### 1.1 Infrastructure Setup (Week 1)
- [ ] **Redis Configuration**
  - Install Redis server
  - Configure connection pooling
  - Set up Redis for caching and queuing
  - Test connection from FastAPI

- [ ] **Celery Configuration**
  - Install Celery and dependencies
  - Configure Celery workers (3 queues: default, ai_processing, file_processing)
  - Set up Celery beat for scheduled tasks
  - Test task execution

- [ ] **Database Updates**
  - Run `database_schema_ai_updated.sql`
  - Verify all tables created successfully
  - Test stored procedures
  - Set up database indexes

- [ ] **File Storage Setup**
  - Configure S3 or local file storage
  - Set up upload directories
  - Test file upload/download

#### 1.2 Admin Dataset Upload API (Week 2)
- [ ] **Upload Endpoint**
  ```python
  POST /api/admin/datasets/upload
  ```
  - Implement multipart file upload
  - File type validation (CSV, Excel, JSON)
  - File size limit enforcement (50MB)
  - Store metadata in `dataset_uploads` table

- [ ] **Progress Tracking**
  ```python
  GET /api/admin/datasets/upload/{upload_id}/progress
  ```
  - Real-time progress updates via Redis
  - WebSocket support for live updates
  - Progress percentage calculation

- [ ] **Report Generation**
  ```python
  GET /api/admin/datasets/upload/{upload_id}/report
  ```
  - Validation report with errors
  - Processing summary
  - Downloadable CSV error report

#### 1.3 Data Processing Pipeline (Week 3)
- [ ] **Validation Service**
  - Schema validation for each file type
  - Data type checks
  - Required field validation
  - Duplicate detection
  - Create validation reports

- [ ] **Cleaning Service**
  - Remove invalid rows
  - Normalize text fields
  - Standardize dates
  - Handle missing values
  - Log cleaning operations

- [ ] **Background Task Implementation**
  ```python
  @celery_app.task(name='upload.process_dataset')
  async def process_dataset_task(upload_id: str)
  ```
  - Async dataset processing
  - Status updates in database
  - Error handling with retry logic
  - Completion notification

### Testing Checklist
- [ ] Upload CSV file with alumni data (1000 rows)
- [ ] Verify validation catches errors
- [ ] Confirm cleaning normalizes data
- [ ] Check progress updates in real-time
- [ ] Validate report generation
- [ ] Test error handling for corrupted files

### Dependencies
```python
# requirements.txt additions
celery==5.3.4
redis==5.0.0
pandas==2.2.0
openpyxl==3.1.2
xlrd==2.0.1
python-multipart==0.0.9
```

---

## üï∏Ô∏è PHASE 2: Skill Graph AI + Engagement Scoring (Weeks 4-6)

### Objectives
- Implement Skill Graph AI with embeddings
- Create skill similarity calculation
- Implement Engagement Scoring Model
- Generate leaderboards

### Deliverables

#### 2.1 Skill Graph AI (Weeks 4-5)

##### 2.1.1 Skill Extraction & Storage
- [ ] **Extract Skills from Database**
  - Query all unique skills from `alumni_profiles`
  - Extract skills from `jobs` table
  - Normalize skill names
  - Store in `skill_graph` table

##### 2.1.2 Embedding Generation
- [ ] **Install sentence-transformers**
  ```bash
  pip install sentence-transformers faiss-cpu
  ```

- [ ] **Generate Embeddings**
  ```python
  from sentence_transformers import SentenceTransformer
  model = SentenceTransformer('all-MiniLM-L6-v2')
  embeddings = model.encode(skills)
  ```

- [ ] **Store in Database**
  - Insert embeddings into `skill_embeddings` table
  - Cache in Redis for fast access

##### 2.1.3 Similarity Calculation
- [ ] **Calculate Pairwise Similarity**
  - Use cosine similarity
  - Build FAISS index for fast search
  - Store top 10 similar skills per skill
  - Insert into `skill_similarities` table

##### 2.1.4 API Endpoints
- [ ] `GET /api/ai/skill-graph/network` - Get full skill network
- [ ] `GET /api/ai/skill-graph/related/{skill}` - Get related skills
- [ ] `POST /api/ai/skill-graph/match-alumni` - Match alumni by skills

##### 2.1.5 Background Task
- [ ] **Skill Graph Update Task**
  ```python
  @celery_app.task(name='ai.update_skill_graph')
  async def update_skill_graph_task()
  ```
  - Runs daily at 2 AM
  - Updates embeddings for new skills
  - Recalculates similarities

#### 2.2 Engagement Scoring Model (Week 6)

##### 2.2.1 Score Calculation Logic
- [ ] **Implement Scoring Algorithm**
  - Profile completeness (20 points)
  - Mentorship activity (30 points)
  - Job portal activity (20 points)
  - Event participation (15 points)
  - Forum activity (15 points)
  - Knowledge sharing (20 points)
  - Platform activity (10 points)

##### 2.2.2 Level Assignment
- [ ] **Define Levels**
  - Beginner: 0-50 points
  - Active: 51-150 points
  - Contributor: 151-300 points
  - Veteran: 301-500 points
  - Legend: 501+ points

##### 2.2.3 API Endpoints
- [ ] `GET /api/ai/engagement/user/{user_id}` - Get user score
- [ ] `GET /api/ai/engagement/leaderboard` - Get leaderboard
- [ ] `GET /api/ai/engagement/history/{user_id}` - Get contribution history
- [ ] `POST /api/ai/engagement/recalculate` - Recalculate scores (admin)

##### 2.2.4 Leaderboard Generation
- [ ] **Calculate Rankings**
  - Update `rank_position` in `engagement_scores`
  - Cache top 100 in Redis sorted set
  - Monthly and yearly leaderboards

##### 2.2.5 Background Task
- [ ] **Engagement Score Update Task**
  ```python
  @celery_app.task(name='ai.update_engagement_scores')
  async def update_engagement_scores_task()
  ```
  - Runs daily at 3 AM
  - Recalculates all user scores
  - Updates leaderboard

### Testing Checklist
- [ ] Generate embeddings for 100 skills
- [ ] Verify similarity scores are accurate
- [ ] Test skill graph API responses
- [ ] Calculate engagement scores for 50 users
- [ ] Verify leaderboard rankings
- [ ] Test real-time score updates

### Dependencies
```python
sentence-transformers==2.3.1
faiss-cpu==1.7.4
scikit-learn==1.4.0
numpy==1.26.0
```

---

## üìà PHASE 3: Career Path Prediction + Talent Heatmap (Weeks 7-10)

### Objectives
- Implement Career Path Prediction Engine with ML
- Create Talent Heatmap with clustering
- Build career transition matrix
- Generate geographic insights

### Deliverables

#### 3.1 Career Path Prediction Engine (Weeks 7-8)

##### 3.1.1 Data Collection & Preparation
- [ ] **Extract Training Data**
  - Query `career_paths` table
  - Extract `experience_timeline` from `alumni_profiles`
  - Parse admin uploaded job market data
  - Create feature matrix

##### 3.1.2 Feature Engineering
- [ ] **Encode Features**
  - Current role (one-hot encoding)
  - Skills (multi-hot encoding)
  - Years of experience (numerical)
  - Education level (categorical)
  - Industry sector (categorical)
  - Company size (categorical)

##### 3.1.3 Transition Matrix Calculation
- [ ] **Calculate Probabilities**
  - Count transitions from role A to role B
  - Calculate P(role_B | role_A)
  - Store in `career_transition_matrix` table
  - Calculate average transition duration

##### 3.1.4 ML Model Training
- [ ] **Train Random Forest Classifier**
  ```python
  from sklearn.ensemble import RandomForestClassifier
  model = RandomForestClassifier(n_estimators=100, max_depth=10)
  model.fit(X_train, y_train)
  ```
  - Train per college dataset
  - Save model to S3
  - Store metadata in `ml_models` table

##### 3.1.5 Prediction Pipeline
- [ ] **Implement Prediction Function**
  - Load user profile
  - Extract features
  - Load trained model
  - Predict top 5 career paths
  - Store in `career_predictions` table

##### 3.1.6 LLM Enhancement
- [ ] **Use Emergent LLM Key**
  - Generate personalized career advice
  - Suggest skills to learn
  - Create learning roadmap

##### 3.1.7 API Endpoints
- [ ] `POST /api/ai/career-path/predict` - Get career predictions
- [ ] `GET /api/ai/career-path/transitions` - Get transition statistics
- [ ] `POST /api/ai/career-path/learning-path` - Get learning path

##### 3.1.8 Background Task
- [ ] **Model Retraining Task**
  ```python
  @celery_app.task(name='ai.retrain_career_model')
  async def retrain_career_model_task(college_id: str = None)
  ```
  - Runs weekly
  - Retrains with new data
  - Updates model version

#### 3.2 Talent Heatmap Intelligence (Weeks 9-10)

##### 3.2.1 Geocoding
- [ ] **Standardize Locations**
  - Extract locations from `alumni_profiles`
  - Convert to lat/long (Google Maps API or geocoder library)
  - Store in `geographic_data` table

##### 3.2.2 Clustering Analysis
- [ ] **Implement DBSCAN Clustering**
  ```python
  from sklearn.cluster import DBSCAN
  clustering = DBSCAN(eps=0.5, min_samples=5, metric='haversine')
  labels = clustering.fit_predict(coords)
  ```
  - Cluster alumni by geographic proximity
  - Calculate cluster statistics
  - Store in `talent_clusters` table

##### 3.2.3 Heatmap Generation
- [ ] **Generate Density Map**
  - Calculate alumni density per grid cell
  - Generate GeoJSON for frontend
  - Color coding by density

##### 3.2.4 Trend Analysis
- [ ] **Track Growth**
  - Calculate location growth rate
  - Identify emerging hubs
  - Predict future distribution

##### 3.2.5 API Endpoints
- [ ] `GET /api/ai/heatmap/global` - Get heatmap data
- [ ] `GET /api/ai/heatmap/cluster/{cluster_id}` - Get cluster details
- [ ] `GET /api/ai/heatmap/emerging-hubs` - Get emerging hubs

##### 3.2.6 Background Task
- [ ] **Heatmap Update Task**
  ```python
  @celery_app.task(name='ai.update_talent_heatmap')
  async def update_talent_heatmap_task()
  ```
  - Runs weekly
  - Recalculates clusters
  - Updates heatmap data

### Testing Checklist
- [ ] Train career model with 500 transition records
- [ ] Verify prediction accuracy (>70%)
- [ ] Test career path API with sample user
- [ ] Cluster 200 alumni locations
- [ ] Verify heatmap generation
- [ ] Test GeoJSON format

### Dependencies
```python
scikit-learn==1.4.0
scipy==1.12.0
geopy==2.4.0  # For geocoding
```

---

## ü™™ PHASE 4: Alumni ID Validation + Capsule Ranking (Weeks 11-13)

### Objectives
- Implement AI-Validated Digital Alumni ID
- Create Knowledge Capsules Ranking Engine
- QR code generation and verification
- Personalized content recommendations

### Deliverables

#### 4.1 AI-Validated Digital Alumni ID (Weeks 11-12)

##### 4.1.1 Rule-Based Validation
- [ ] **Profile Completeness Check**
  - Verify required fields
  - Check profile completion >= 80%
  - Validate email domain

##### 4.1.2 Duplicate Detection
- [ ] **Fuzzy Name Matching**
  ```python
  from Levenshtein import distance
  similarity = 1 - (distance(name1, name2) / max(len(name1), len(name2)))
  ```
  - Check for similar names
  - Batch year + department match
  - Flag for manual review

##### 4.1.3 QR Code Generation
- [ ] **Implement QR System**
  ```python
  import qrcode
  from cryptography.fernet import Fernet
  ```
  - Encrypt card data (AES-256)
  - Generate QR code image
  - Create digital ID card (PIL)
  - Upload to S3

##### 4.1.4 Card Verification
- [ ] **Verification Pipeline**
  - Decode QR code
  - Decrypt payload
  - Verify signature
  - Check expiry date
  - Log verification

##### 4.1.5 API Endpoints
- [ ] `POST /api/ai/alumni-card/generate` - Generate card
- [ ] `POST /api/ai/alumni-card/verify` - Verify card
- [ ] `GET /api/ai/alumni-card/{user_id}` - Get card details

#### 4.2 Knowledge Capsules Ranking Engine (Week 13)

##### 4.2.1 Scoring Components
- [ ] **Skill Match Score**
  - Jaccard similarity between user skills and capsule tags
  - Weight: 30%

- [ ] **Engagement Score**
  - Normalize views, likes, bookmarks
  - Weight: 25%

- [ ] **Credibility Score**
  - Author's engagement score
  - Total capsules published
  - Verification status
  - Weight: 20%

- [ ] **Recency Score**
  - Exponential decay: e^(-0.01 * age_days)
  - Weight: 15%

- [ ] **Content Relevance** (LLM-based)
  - Use Emergent LLM Key
  - Rate relevance 0-1
  - Weight: 10%

##### 4.2.2 Ranking Calculation
- [ ] **Implement Ranking Algorithm**
  ```python
  final_score = (0.30 * skill_match + 
                 0.25 * engagement + 
                 0.20 * credibility + 
                 0.15 * recency + 
                 0.10 * content_relevance)
  ```

##### 4.2.3 API Endpoints
- [ ] `GET /api/ai/knowledge/ranked` - Get personalized capsules
- [ ] `GET /api/ai/knowledge/trending` - Get trending capsules
- [ ] `POST /api/ai/knowledge/learning-path` - Get learning path

##### 4.2.4 Background Task
- [ ] **Ranking Update Task**
  ```python
  @celery_app.task(name='ai.update_capsule_rankings')
  async def update_capsule_rankings_task()
  ```
  - Runs daily at 4 AM
  - Recalculates rankings for all users
  - Caches results in Redis

### Testing Checklist
- [ ] Generate 10 alumni ID cards
- [ ] Verify QR codes scan correctly
- [ ] Test duplicate detection
- [ ] Calculate rankings for 20 capsules
- [ ] Verify personalized recommendations
- [ ] Test LLM integration

### Dependencies
```python
qrcode==7.4.2
Pillow==10.2.0
reportlab==4.0.0
python-Levenshtein==0.21.1
cryptography==42.0.8
```

---

## ‚ö° PHASE 5: Optimization & Testing (Weeks 14-15)

### Objectives
- Optimize API performance
- Implement comprehensive caching
- Conduct load testing
- Deploy to production

### Deliverables

#### 5.1 Performance Optimization

##### 5.1.1 API Caching
- [ ] **Implement Redis Caching**
  - Cache API responses (TTL: 5-10 min)
  - Cache AI model outputs (TTL: 24 hours)
  - Cache leaderboards (TTL: 1 hour)

##### 5.1.2 Database Optimization
- [ ] **Query Optimization**
  - Add missing indexes
  - Optimize complex joins
  - Use database views
  - Implement read replicas

##### 5.1.3 Background Job Optimization
- [ ] **Celery Tuning**
  - Optimize worker pool size
  - Implement task priorities
  - Add retry logic
  - Set timeouts

#### 5.2 Comprehensive Testing

##### 5.2.1 Unit Tests
- [ ] Test AI algorithms
- [ ] Test API endpoints
- [ ] Test background tasks
- [ ] Test data validation

##### 5.2.2 Integration Tests
- [ ] Test full dataset upload pipeline
- [ ] Test AI pipeline triggering
- [ ] Test frontend-backend integration

##### 5.2.3 Load Testing
- [ ] **Use Locust or Apache JMeter**
  - 1000 concurrent users
  - API response time < 500ms
  - AI processing time benchmarks

##### 5.2.4 Security Testing
- [ ] Test JWT authentication
- [ ] Test rate limiting
- [ ] Test input validation
- [ ] Test file upload security

#### 5.3 Monitoring & Logging

##### 5.3.1 Prometheus Metrics
- [ ] Set up Prometheus
- [ ] Add custom metrics
- [ ] Create Grafana dashboards

##### 5.3.2 Logging
- [ ] Configure structured logging
- [ ] Set up log aggregation
- [ ] Configure Sentry for error tracking

#### 5.4 Documentation

##### 5.4.1 API Documentation
- [ ] Generate OpenAPI/Swagger docs
- [ ] Create Postman collection
- [ ] Write integration guides

##### 5.4.2 Deployment Documentation
- [ ] Write deployment guide
- [ ] Document environment variables
- [ ] Create runbooks

#### 5.5 Production Deployment

##### 5.5.1 Pre-deployment
- [ ] Code review
- [ ] Security audit
- [ ] Performance benchmarks

##### 5.5.2 Deployment
- [ ] Deploy to staging
- [ ] Run smoke tests
- [ ] Deploy to production
- [ ] Monitor metrics

### Testing Checklist
- [ ] All unit tests passing
- [ ] Integration tests passing
- [ ] Load test completed (1000 users)
- [ ] Security scan passed
- [ ] Documentation reviewed
- [ ] Production deployment successful

---

## üìä Success Metrics

### Performance KPIs

| Metric | Target | Measurement |
|--------|--------|-------------|
| API Response Time | < 100ms (cached) | P95 latency |
| API Response Time | < 500ms (uncached) | P95 latency |
| Skill Graph Update | < 5 minutes | Task duration |
| Career Prediction | < 2 seconds | API response |
| Heatmap Clustering | < 10 minutes | Task duration |
| Dataset Processing | 1000 rows/min | Processing rate |
| Uptime | 99.9% | Monthly average |

### AI Model Accuracy

| Model | Target Accuracy | Measurement |
|-------|----------------|-------------|
| Career Prediction | > 70% | Classification accuracy |
| Skill Similarity | > 80% | Human validation |
| Alumni ID Validation | > 95% | Duplicate detection rate |
| Capsule Ranking | > 75% | User satisfaction score |

---

## üîÑ Continuous Improvement

### Monthly Tasks
- [ ] Retrain ML models with new data
- [ ] Review and optimize slow queries
- [ ] Update skill embeddings
- [ ] Recalculate engagement scores

### Quarterly Tasks
- [ ] Model accuracy evaluation
- [ ] A/B testing for ranking algorithms
- [ ] User feedback analysis
- [ ] Performance optimization review

---

## üìû Team Responsibilities

### Backend Team
- API development
- Database schema updates
- Background job implementation

### ML Team
- AI model development
- Algorithm optimization
- Model training and evaluation

### DevOps Team
- Infrastructure setup
- Deployment automation
- Monitoring and alerting

### Frontend Team
- API integration
- UI for admin upload
- Visualization components

---

## üéØ Critical Success Factors

1. **Data Quality**: Ensure uploaded datasets are clean and validated
2. **Model Accuracy**: ML models must achieve >70% accuracy
3. **Performance**: APIs must respond within 500ms
4. **Scalability**: System must handle 10,000+ users
5. **Monitoring**: Comprehensive logging and alerting
6. **Documentation**: Clear API docs and deployment guides

---

## üìù Next Steps

1. **Review this roadmap** with technical team
2. **Assign responsibilities** to team members
3. **Set up project tracking** (Jira/GitHub Projects)
4. **Begin Phase 1** infrastructure setup
5. **Schedule weekly sync meetings** for progress updates

---

**Document Version**: 1.0  
**Created**: January 2025  
**Owner**: Technical Architecture Team  
**Status**: Ready for Implementation

---

**FOR QUESTIONS OR CLARIFICATIONS, CONTACT:**
- Technical Lead: [Contact]
- ML Lead: [Contact]
- DevOps Lead: [Contact]
